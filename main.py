"""
Saudi Labor Law Chatbot - FastAPI Backend
OPTIMIZED VERSION - Persistent Embeddings (No Re-processing)
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any
import os
import re
import hashlib

import config
from vectordb import VectorDB
from document_processor import DocumentProcessor
from sentence_transformers import SentenceTransformer
from groq import Groq

# ============================================================================
# FASTAPI APP SETUP
# ============================================================================

app = FastAPI(title="Saudi Labor Law Chatbot API")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# GLOBAL VARIABLES
# ============================================================================

vectordb = None
embedding_model = None
groq_client = None

# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class QuestionRequest(BaseModel):
    question: str

class AnswerResponse(BaseModel):
    answer: str
    articles: List[str]
    context_chunks: List[Dict[str, Any]]

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_pdf_hash(pdf_path: str) -> str:
    """Calculate MD5 hash of PDF to detect changes"""
    md5 = hashlib.md5()
    with open(pdf_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5.update(chunk)
    return md5.hexdigest()

def collection_exists_and_valid(vectordb: VectorDB, collection_name: str, pdf_hash: str) -> bool:
    """Check if collection exists and matches current PDF"""
    try:
        # Check if collection exists
        collection_info = vectordb.client.get_collection(collection_name)
        
        # Check if it has the PDF hash stored
        stored_hash = collection_info.config.params.get('metadata', {}).get('pdf_hash')
        
        if stored_hash == pdf_hash:
            # Check if collection has data
            count = vectordb.client.count(collection_name)
            return count.count > 0
        
        return False
    except:
        return False

# ============================================================================
# STARTUP EVENT
# ============================================================================

@app.on_event("startup")
async def startup_event():
    global vectordb, embedding_model, groq_client
    
    print("\n" + "="*70)
    print("ğŸš€ STARTING SAUDI LABOR LAW CHATBOT (OPTIMIZED)")
    print("="*70)
    
    # ========================================================================
    # STEP 1: Initialize Groq Client
    # ========================================================================
    print("\nğŸ“¡ Initializing Groq API Client...")
    try:
        groq_client = Groq(api_key=config.GROQ_API_KEY)
        print(f"âœ… Groq client initialized with model: {config.GROQ_MODEL}")
    except Exception as e:
        print(f"âŒ Failed to initialize Groq: {e}")
        raise
    
    # ========================================================================
    # STEP 2: Connect to Vector Database
    # ========================================================================
    print("\nğŸ’¾ Setting up Vector Database...")
    vectordb = VectorDB(config.QDRANT_HOST, config.QDRANT_PORT)
    vectordb.connect()
    
    # ========================================================================
    # STEP 3: Check if we need to rebuild
    # ========================================================================
    pdf_path = "data/saudi_labor_law.pdf"
    
    if not os.path.exists(pdf_path):
        print(f"\nâš ï¸  WARNING: PDF not found at {pdf_path}")
        return
    
    # Calculate current PDF hash
    current_pdf_hash = get_pdf_hash(pdf_path)
    print(f"\nğŸ” PDF Hash: {current_pdf_hash[:8]}...")
    
    # Check if collection exists and is up-to-date
    if collection_exists_and_valid(vectordb, config.COLLECTION_NAME, current_pdf_hash):
        print("\nâœ… FOUND EXISTING EMBEDDINGS - SKIPPING REBUILD!")
        print("   (Collection is up-to-date)")
        
        # Just load the embedding model for queries
        print("\nğŸ”¤ Loading Embedding Model...")
        embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)
        print("âœ… Embedding model loaded!")
        
        print("\n" + "="*70)
        print("âœ… SYSTEM READY! (Using cached embeddings)")
        print("="*70)
        print(f"ğŸ¤– Using: {config.GROQ_MODEL}")
        print(f"ğŸŒ API: http://localhost:8000")
        print("="*70 + "\n")
        return
    
    # ========================================================================
    # STEP 4: Rebuild needed - Delete old collection
    # ========================================================================
    print("\nğŸ”„ REBUILD NEEDED (PDF changed or first run)")
    
    try:
        vectordb.client.delete_collection(config.COLLECTION_NAME)
        print("ğŸ—‘ï¸  Deleted old collection")
    except:
        print("â„¹ï¸  No old collection to delete")
    
    # ========================================================================
    # STEP 5: Load Embedding Model
    # ========================================================================
    print("\nğŸ”¤ Loading Embedding Model...")
    print(f"   Model: {config.EMBEDDING_MODEL}")
    embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)
    print("âœ… Embedding model loaded!")
    
    # ========================================================================
    # STEP 6: Process PDF and Index
    # ========================================================================
    try:
        # Create collection with PDF hash in metadata
        print("\nğŸ“¦ Creating collection with metadata...")
        vectordb.create_collection(
            config.COLLECTION_NAME, 
            config.EMBEDDING_SIZE,
            metadata={'pdf_hash': current_pdf_hash}
        )
        
        # Process PDF using DocumentProcessor
        print("\nğŸ“„ Processing PDF with DocumentProcessor...")
        processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)
        chunks = processor.process(pdf_path)
        
        if not chunks:
            print("âŒ No chunks extracted")
            return
        
        print(f"âœ… Created {len(chunks)} chunks")
        
        # Embed chunks
        print("\nğŸ”„ Embedding chunks...")
        texts = [chunk['text'] for chunk in chunks]
        embeddings = embedding_model.encode(texts, show_progress_bar=True)
        
        # Prepare payloads
        payloads = [
            {
                'text': chunk['text'],
                'article_number': chunk.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                'chunk_id': str(chunk['chunk_id'])
            }
            for chunk in chunks
        ]
        
        # Insert into Qdrant
        print("\nğŸ’¾ Inserting into Vector Database...")
        vectordb.insert(
            config.COLLECTION_NAME,
            embeddings.tolist(),
            payloads
        )
        
        print("\n" + "="*70)
        print("âœ… SYSTEM READY! (Embeddings saved)")
        print("="*70)
        print(f"ğŸ“Š Indexed {len(chunks)} chunks")
        print(f"ğŸ¤– Using: {config.GROQ_MODEL}")
        print(f"ğŸŒ API: http://localhost:8000")
        print(f"ğŸ’¾ Next startup will be INSTANT (using cached embeddings)")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        raise

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/")
async def serve_ui():
    """Serve the HTML UI"""
    return FileResponse("index.html")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model": config.GROQ_MODEL,
        "vector_db": "connected" if vectordb else "not connected"
    }

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    """
    Main endpoint - Process user question
    """
    
    question = request.question.strip()
    
    if not question:
        raise HTTPException(status_code=400, detail="Question cannot be empty")
    
    print(f"\n{'='*70}")
    print(f"â“ Question: {question}")
    print(f"{'='*70}")
    
    try:
        # ====================================================================
        # STEP 1: Embed question
        # ====================================================================
        print("ğŸ” Embedding question...")
        question_embedding = embedding_model.encode([question])[0].tolist()
        
        # ====================================================================
        # STEP 2: Search vector DB
        # ====================================================================
        print("ğŸ” Searching vector database...")
        results = vectordb.search(
            collection_name=config.COLLECTION_NAME,
            query_vector=question_embedding,
            limit=3
        )
        
        print(f"âœ… Found {len(results)} relevant chunks")
        
        # ====================================================================
        # STEP 3: Build context
        # ====================================================================
        context_parts = []
        articles_mentioned = set()
        
        for i, result in enumerate(results, 1):
            text = result['text']
            article = result.get('article', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            
            # Extract article mentions from text
            article_matches = re.findall(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+[^\n:.]{1,50}', text[:300])
            for match in article_matches:
                articles_mentioned.add(match.strip())
            
            context_parts.append(f"[Ù…Ù‚ØªØ·Ù {i}]:\n{text}\n")
        
        context = "\n".join(context_parts)
        
        # ====================================================================
        # STEP 4: Build prompt
        # ====================================================================
        system_prompt  = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.
Ù…Ù‡Ù…ØªÙƒ: ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø© Ù„Ù„Ù…ÙˆØ¸ÙÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ ÙÙ‚Ø·.

 Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø³Ø§Ø³ÙŠØ©:
- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø· (Ù…Ù…Ù†ÙˆØ¹ Ø£ÙŠ Ù„ØºØ© Ø£Ø®Ø±Ù‰)
- Ø§Ø³ØªÙ†Ø¯ ÙÙ‚Ø· Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…
- Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø¹Ù†Ø¯ ÙƒÙ„ Ø§Ø³ØªØ´Ù‡Ø§Ø¯
- Ø£Ø¬Ø¨ Ø¨Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬ÙˆØ¯ ÙÙ‚Ø·ØŒ Ù„Ø§ ØªØ°ÙƒØ± Ù…Ø§ Ù‡Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª: "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª" Ø£Ùˆ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø´Ø§Ø±Ø§Øª" Ø£Ùˆ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ Ø£Ø®Ø±Ù‰"

 Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ø°ÙƒØ± ÙÙ‚Ø· Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬ÙˆØ¯):
1. Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©)
2. Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)
3. Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø®Ø§ØµØ© (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)
4. Ø±Ø¨Ø· Ø§Ù„Ù…ÙˆØ§Ø¯ Ø°Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)

 Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
- ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø¨Ø§Ø´Ø± ÙˆÙ…Ø®ØªØµØ±
- Ù„ØºØ© Ø¨Ø³ÙŠØ·Ø© ÙŠÙÙ‡Ù…Ù‡Ø§ ØºÙŠØ± Ø§Ù„Ù…ØªØ®ØµØµ
- ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©
- Ù„Ø§ Ø­Ø´Ùˆ ÙˆÙ„Ø§ ØªÙƒØ±Ø§Ø±

---
### Ù…Ø«Ø§Ù„:
**Ø³Ø¤Ø§Ù„:** ÙƒÙ… Ù…Ø¯Ø© ÙØªØ±Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø©ØŸ

**Ø¥Ø¬Ø§Ø¨Ø©:**
ÙˆÙÙ‚Ù‹Ø§ Ù„Ù€ **Ø§Ù„Ù…Ø§Ø¯Ø© (53)** Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØŒ ÙØªØ±Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù„Ø§ ØªØ²ÙŠØ¯ Ø¹Ù„Ù‰ **90 ÙŠÙˆÙ…Ù‹Ø§**ØŒ 
ÙˆÙŠØ¬Ø¨ Ø§Ù„Ù†Øµ Ø¹Ù„ÙŠÙ‡Ø§ ØµØ±Ø§Ø­Ø© ÙÙŠ Ø§Ù„Ø¹Ù‚Ø¯.

ÙˆÙŠØ¬ÙˆØ² Ø¨Ø§Ù„Ø§ØªÙØ§Ù‚ Ø§Ù„Ù…ÙƒØªÙˆØ¨ **ØªÙ…Ø¯ÙŠØ¯Ù‡Ø§** Ù„Ù€ **180 ÙŠÙˆÙ…Ù‹Ø§ ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰**.

Ù„Ø§ ØªØ¯Ø®Ù„ Ø¶Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨:
- Ø¥Ø¬Ø§Ø²Ø© Ø¹ÙŠØ¯ÙŠ Ø§Ù„ÙØ·Ø± ÙˆØ§Ù„Ø£Ø¶Ø­Ù‰
- Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ù…Ø±Ø¶ÙŠØ©

ÙŠØ­Ù‚ Ù„Ù„Ø·Ø±ÙÙŠÙ† Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯ Ø®Ù„Ø§Ù„Ù‡Ø§ØŒ Ù…Ø§ Ù„Ù… ÙŠÙ†Øµ Ø§Ù„Ø¹Ù‚Ø¯ Ø¹Ù„Ù‰ Ø®Ù„Ø§Ù Ø°Ù„Ùƒ.

ÙˆØ¨Ø­Ø³Ø¨ **Ø§Ù„Ù…Ø§Ø¯Ø© (54)**ØŒ Ù„Ø§ ÙŠØ¬ÙˆØ² ØªÙƒØ±Ø§Ø± ÙØªØ±Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù„Ø¯Ù‰ Ù†ÙØ³ ØµØ§Ø­Ø¨ Ø§Ù„Ø¹Ù…Ù„ØŒ 
Ø¥Ù„Ø§ ÙÙŠ Ù…Ù‡Ù†Ø© Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¨Ø¹Ø¯ Ù…Ø±ÙˆØ± 6 Ø£Ø´Ù‡Ø± Ù…Ù† Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.

Ù„Ø§ ÙŠØ³ØªØ­Ù‚ Ø£ÙŠ Ø·Ø±Ù ØªØ¹ÙˆÙŠØ¶Ù‹Ø§ Ø£Ùˆ Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø®Ø¯Ù…Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¬Ø±Ø¨Ø©.

---
 ØªØ°ÙƒØ±: Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬ÙˆØ¯ØŒ ÙˆØ§Ø®ØªÙ… Ø¹Ù†Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©.
"""

        user_prompt = f"""Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ:
{context}

Ø³Ø¤Ø§Ù„: {question}

Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©."""

        # ====================================================================
        # STEP 5: Call Groq API
        # ====================================================================
        print("ğŸ¤– Generating answer...")
        
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            model=config.GROQ_MODEL,
            temperature=0.3,
            max_tokens=1000,
            top_p=0.9
        )
        
        answer = chat_completion.choices[0].message.content
        
        print(f"âœ… Answer generated")
        
        # ====================================================================
        # STEP 6: Extract article citations from answer
        # ====================================================================
        answer_articles = re.findall(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+[^\n:.ØŒ]{1,50}', answer)
        all_articles = list(set(articles_mentioned) | set(answer_articles))
        
        # Clean up article list
        cleaned_articles = []
        for art in all_articles:
            # Extract just the article reference
            art_clean = art.replace('Ø§Ù„Ù…Ø§Ø¯Ø© ', '').strip()
            if art_clean and art_clean != 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯':
                cleaned_articles.append(f"Ø§Ù„Ù…Ø§Ø¯Ø© {art_clean}")
        
        print(f"ğŸ“š Articles cited: {cleaned_articles}")
        
        # ====================================================================
        # STEP 7: Return response
        # ====================================================================
        return AnswerResponse(
            answer=answer,
            articles=cleaned_articles[:5],  # Limit to 5
            context_chunks=[
                {
                    'text': r['text'][:200] + "...",
                    'article': r.get('article', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                    'score': round(r['score'], 3)
                }
                for r in results
            ]
        )
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# FORCE REBUILD ENDPOINT (Optional - for manual rebuild)
# ============================================================================

@app.post("/rebuild")
async def force_rebuild():
    """Force rebuild of embeddings (useful for testing)"""
    try:
        vectordb.client.delete_collection(config.COLLECTION_NAME)
        await startup_event()
        return {"status": "success", "message": "Embeddings rebuilt"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# SHUTDOWN EVENT
# ============================================================================

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    if vectordb:
        vectordb.close()
    print("ğŸ‘‹ Server shutdown complete")

# ============================================================================
# RUN SERVER
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


# """
# Saudi Labor Law Chatbot - FastAPI Backend
# SIMPLIFIED VERSION - NO COMPLEX ARTICLE EXTRACTION
# """

# from fastapi import FastAPI, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import FileResponse
# from pydantic import BaseModel
# from typing import List, Dict, Any
# import os
# import re

# import config
# from vectordb import VectorDB
# from document_processor import DocumentProcessor
# from sentence_transformers import SentenceTransformer
# from groq import Groq

# # ============================================================================
# # FASTAPI APP SETUP
# # ============================================================================

# app = FastAPI(title="Saudi Labor Law Chatbot API")

# # Enable CORS
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # ============================================================================
# # GLOBAL VARIABLES
# # ============================================================================

# vectordb = None
# embedding_model = None
# groq_client = None

# # ============================================================================
# # PYDANTIC MODELS
# # ============================================================================

# class QuestionRequest(BaseModel):
#     question: str

# class AnswerResponse(BaseModel):
#     answer: str
#     articles: List[str]
#     context_chunks: List[Dict[str, Any]]

# # ============================================================================
# # STARTUP EVENT
# # ============================================================================

# @app.on_event("startup")
# async def startup_event():
#     global vectordb, embedding_model, groq_client
    
#     print("\n" + "="*70)
#     print("ğŸš€ STARTING SAUDI LABOR LAW CHATBOT")
#     print("="*70)
    
#     # ========================================================================
#     # STEP 1: Initialize Groq Client
#     # ========================================================================
#     print("\nğŸ“¡ Initializing Groq API Client...")
#     try:
#         groq_client = Groq(api_key=config.GROQ_API_KEY)
#         print(f"âœ… Groq client initialized with model: {config.GROQ_MODEL}")
#     except Exception as e:
#         print(f"âŒ Failed to initialize Groq: {e}")
#         raise
    
#     # ========================================================================
#     # STEP 2: Connect to Vector Database
#     # ========================================================================
#     print("\nğŸ’¾ Setting up Vector Database...")
#     vectordb = VectorDB(config.QDRANT_HOST, config.QDRANT_PORT)
#     vectordb.connect()
    
#     # DELETE OLD COLLECTION (Fresh start)
#     try:
#         vectordb.client.delete_collection(config.COLLECTION_NAME)
#         print("ğŸ—‘ï¸  Deleted old collection")
#     except:
#         print("â„¹ï¸  No old collection to delete")
    
#     # ========================================================================
#     # STEP 3: Load Embedding Model
#     # ========================================================================
#     print("\nğŸ”¤ Loading Embedding Model...")
#     print(f"   Model: {config.EMBEDDING_MODEL}")
#     embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)
#     print("âœ… Embedding model loaded!")
    
#     # ========================================================================
#     # STEP 4: Process PDF and Index
#     # ========================================================================
#     pdf_path = "data/saudi_labor_law.pdf"
    
#     if not os.path.exists(pdf_path):
#         print(f"\nâš ï¸  WARNING: PDF not found at {pdf_path}")
#         return
    
#     try:
#         # Create collection
#         vectordb.create_collection(config.COLLECTION_NAME, config.EMBEDDING_SIZE)
        
#         # Process PDF using YOUR DocumentProcessor class
#         print("\nğŸ“„ Processing PDF with DocumentProcessor...")
#         processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)
#         chunks = processor.process(pdf_path)
        
#         if not chunks:
#             print("âŒ No chunks extracted")
#             return
        
#         print(f"âœ… Created {len(chunks)} chunks")
        
#         # Embed chunks
#         print("\nğŸ”„ Embedding chunks...")
#         texts = [chunk['text'] for chunk in chunks]
#         embeddings = embedding_model.encode(texts, show_progress_bar=True)
        
#         # Prepare payloads
#         payloads = [
#             {
#                 'text': chunk['text'],
#                 'article_number': chunk.get('article_number', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
#                 'chunk_id': str(chunk['chunk_id'])
#             }
#             for chunk in chunks
#         ]
        
#         # Insert into Qdrant
#         print("\nğŸ’¾ Inserting into Vector Database...")
#         vectordb.insert(
#             config.COLLECTION_NAME,
#             embeddings.tolist(),
#             payloads
#         )
        
#         print("\n" + "="*70)
#         print("âœ… SYSTEM READY!")
#         print("="*70)
#         print(f"ğŸ“Š Indexed {len(chunks)} chunks")
#         print(f"ğŸ¤– Using: {config.GROQ_MODEL}")
#         print(f"ğŸŒ API: http://localhost:8000")
#         print("="*70 + "\n")
        
#     except Exception as e:
#         print(f"\nâŒ Error: {e}")
#         raise

# # ============================================================================
# # API ENDPOINTS
# # ============================================================================

# @app.get("/")
# async def serve_ui():
#     """Serve the HTML UI"""
#     return FileResponse("index.html")

# @app.get("/health")
# async def health_check():
#     """Health check endpoint"""
#     return {
#         "status": "healthy",
#         "model": config.GROQ_MODEL,
#         "vector_db": "connected" if vectordb else "not connected"
#     }

# @app.post("/ask", response_model=AnswerResponse)
# async def ask_question(request: QuestionRequest):
#     """
#     Main endpoint - Process user question
#     """
    
#     question = request.question.strip()
    
#     if not question:
#         raise HTTPException(status_code=400, detail="Question cannot be empty")
    
#     print(f"\n{'='*70}")
#     print(f"â“ Question: {question}")
#     print(f"{'='*70}")
    
#     try:
#         # ====================================================================
#         # STEP 1: Embed question
#         # ====================================================================
#         print("ğŸ” Embedding question...")
#         question_embedding = embedding_model.encode([question])[0].tolist()
        
#         # ====================================================================
#         # STEP 2: Search vector DB
#         # ====================================================================
#         print("ğŸ” Searching vector database...")
#         results = vectordb.search(
#             collection_name=config.COLLECTION_NAME,
#             query_vector=question_embedding,
#             limit=3
#         )
        
#         print(f"âœ… Found {len(results)} relevant chunks")
        
#         # ====================================================================
#         # STEP 3: Build context
#         # ====================================================================
#         context_parts = []
#         articles_mentioned = set()
        
#         for i, result in enumerate(results, 1):
#             text = result['text']
#             article = result.get('article', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            
#             # Extract article mentions from text
#             article_matches = re.findall(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+[^\n:.]{1,50}', text[:300])
#             for match in article_matches:
#                 articles_mentioned.add(match.strip())
            
#             context_parts.append(f"[Ù…Ù‚ØªØ·Ù {i}]:\n{text}\n")
        
#         context = "\n".join(context_parts)
        
#         # ====================================================================
#         # STEP 4: Build prompt
#         # ====================================================================
#         system_prompt  = """
# Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ.
# Ù…Ù‡Ù…ØªÙƒ: ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙØµÙ„Ø© Ù„Ù„Ù…ÙˆØ¸ÙÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ ÙÙ‚Ø·.

#  Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø³Ø§Ø³ÙŠØ©:
# - Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø· (Ù…Ù…Ù†ÙˆØ¹ Ø£ÙŠ Ù„ØºØ© Ø£Ø®Ø±Ù‰)
# - Ø§Ø³ØªÙ†Ø¯ ÙÙ‚Ø· Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…
# - Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø¹Ù†Ø¯ ÙƒÙ„ Ø§Ø³ØªØ´Ù‡Ø§Ø¯
# - Ø£Ø¬Ø¨ Ø¨Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬ÙˆØ¯ ÙÙ‚Ø·ØŒ Ù„Ø§ ØªØ°ÙƒØ± Ù…Ø§ Ù‡Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
# - Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª: "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª" Ø£Ùˆ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø´Ø§Ø±Ø§Øª" Ø£Ùˆ "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ Ø£Ø®Ø±Ù‰"

#  Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø§Ø°ÙƒØ± ÙÙ‚Ø· Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬ÙˆØ¯):
# 1. Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©)
# 2. Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)
# 3. Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø®Ø§ØµØ© (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)
# 4. Ø±Ø¨Ø· Ø§Ù„Ù…ÙˆØ§Ø¯ Ø°Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)

#  Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
# - ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø¨Ø§Ø´Ø± ÙˆÙ…Ø®ØªØµØ±
# - Ù„ØºØ© Ø¨Ø³ÙŠØ·Ø© ÙŠÙÙ‡Ù…Ù‡Ø§ ØºÙŠØ± Ø§Ù„Ù…ØªØ®ØµØµ
# - ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©
# - Ù„Ø§ Ø­Ø´Ùˆ ÙˆÙ„Ø§ ØªÙƒØ±Ø§Ø±

# ---
# ### Ù…Ø«Ø§Ù„:
# **Ø³Ø¤Ø§Ù„:** ÙƒÙ… Ù…Ø¯Ø© ÙØªØ±Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø©ØŸ

# **Ø¥Ø¬Ø§Ø¨Ø©:**
# ÙˆÙÙ‚Ù‹Ø§ Ù„Ù€ **Ø§Ù„Ù…Ø§Ø¯Ø© (53)** Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØŒ ÙØªØ±Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù„Ø§ ØªØ²ÙŠØ¯ Ø¹Ù„Ù‰ **90 ÙŠÙˆÙ…Ù‹Ø§**ØŒ 
# ÙˆÙŠØ¬Ø¨ Ø§Ù„Ù†Øµ Ø¹Ù„ÙŠÙ‡Ø§ ØµØ±Ø§Ø­Ø© ÙÙŠ Ø§Ù„Ø¹Ù‚Ø¯.

# ÙˆÙŠØ¬ÙˆØ² Ø¨Ø§Ù„Ø§ØªÙØ§Ù‚ Ø§Ù„Ù…ÙƒØªÙˆØ¨ **ØªÙ…Ø¯ÙŠØ¯Ù‡Ø§** Ù„Ù€ **180 ÙŠÙˆÙ…Ù‹Ø§ ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰**.

# Ù„Ø§ ØªØ¯Ø®Ù„ Ø¶Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨:
# - Ø¥Ø¬Ø§Ø²Ø© Ø¹ÙŠØ¯ÙŠ Ø§Ù„ÙØ·Ø± ÙˆØ§Ù„Ø£Ø¶Ø­Ù‰
# - Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ù…Ø±Ø¶ÙŠØ©

# ÙŠØ­Ù‚ Ù„Ù„Ø·Ø±ÙÙŠÙ† Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯ Ø®Ù„Ø§Ù„Ù‡Ø§ØŒ Ù…Ø§ Ù„Ù… ÙŠÙ†Øµ Ø§Ù„Ø¹Ù‚Ø¯ Ø¹Ù„Ù‰ Ø®Ù„Ø§Ù Ø°Ù„Ùƒ.

# ÙˆØ¨Ø­Ø³Ø¨ **Ø§Ù„Ù…Ø§Ø¯Ø© (54)**ØŒ Ù„Ø§ ÙŠØ¬ÙˆØ² ØªÙƒØ±Ø§Ø± ÙØªØ±Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù„Ø¯Ù‰ Ù†ÙØ³ ØµØ§Ø­Ø¨ Ø§Ù„Ø¹Ù…Ù„ØŒ 
# Ø¥Ù„Ø§ ÙÙŠ Ù…Ù‡Ù†Ø© Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¨Ø¹Ø¯ Ù…Ø±ÙˆØ± 6 Ø£Ø´Ù‡Ø± Ù…Ù† Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.

# Ù„Ø§ ÙŠØ³ØªØ­Ù‚ Ø£ÙŠ Ø·Ø±Ù ØªØ¹ÙˆÙŠØ¶Ù‹Ø§ Ø£Ùˆ Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø®Ø¯Ù…Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¬Ø±Ø¨Ø©.

# ---
#  ØªØ°ÙƒØ±: Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬ÙˆØ¯ØŒ ÙˆØ§Ø®ØªÙ… Ø¹Ù†Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©.
# """

#         user_prompt = f"""Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ:
# {context}

# Ø³Ø¤Ø§Ù„: {question}

# Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ Ø°ÙƒØ± Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©."""

#         # ====================================================================
#         # STEP 5: Call Groq API
#         # ====================================================================
#         print(" Generating answer...")
        
#         chat_completion = groq_client.chat.completions.create(
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             model=config.GROQ_MODEL,
#             temperature=0.3,
#             max_tokens=1000,
#             top_p=0.9
#         )
        
#         answer = chat_completion.choices[0].message.content
        
#         print(f" Answer generated")
        
#         # ====================================================================
#         # STEP 6: Extract article citations from answer
#         # ====================================================================
#         answer_articles = re.findall(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+[^\n:.ØŒ]{1,50}', answer)
#         all_articles = list(set(articles_mentioned) | set(answer_articles))
        
#         # Clean up article list
#         cleaned_articles = []
#         for art in all_articles:
#             # Extract just the article reference
#             art_clean = art.replace('Ø§Ù„Ù…Ø§Ø¯Ø© ', '').strip()
#             if art_clean and art_clean != 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯':
#                 cleaned_articles.append(f"Ø§Ù„Ù…Ø§Ø¯Ø© {art_clean}")
        
#         print(f" Articles cited: {cleaned_articles}")
        
#         # ====================================================================
#         # STEP 7: Return response
#         # ====================================================================
#         return AnswerResponse(
#             answer=answer,
#             articles=cleaned_articles[:5],  # Limit to 5
#             context_chunks=[
#                 {
#                     'text': r['text'][:200] + "...",
#                     'article': r.get('article', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
#                     'score': round(r['score'], 3)
#                 }
#                 for r in results
#             ]
#         )
        
#     except Exception as e:
#         print(f"âŒ Error: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# # ============================================================================
# # SHUTDOWN EVENT
# # ============================================================================

# @app.on_event("shutdown")
# async def shutdown_event():
#     """Cleanup on shutdown"""
#     if vectordb:
#         vectordb.close()
#     print(" Server shutdown complete")

# # ============================================================================
# # RUN SERVER
# # ============================================================================

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)


